\chapter{Machine Learning Background}
\label{c:ml-background}

\note{Machine learning stuff, around 10 pages ..?}

\section{Probabilities}

Random variable

Marginalisation

Inference

\section{Mixture model clustering}

% Preview source code from paragraph 15 to 16

In the clustering problem, we are presented with a list of feature vectors as input, and our goal is to separate those data points (features) into groups. Clustering is an instance of unsupervised learning where the learning algorithm tries to find hidden structure in unlabeled data -- in constrast to supervised learning, where each data point comes with a class label. Many clustering algorithms exist, the simplest of which is k-means clustering. In k-means, we assume that the data contains a fixed set of $K$ clusters. Features are then assigned to the nearest cluster centroids based on some distance function. Each cluster centroid is updated by computing the average of all the features assigned to it. This process is repeated until convergence. 

An alternative way to cluster data is through statistical model-based clustering. In mixture model clustering, each cluster is represented by a statistical distribution. The normal (Gaussian) distribution is commonly used to model continuous data, while the multinomial distribution is frequently used to model discrete data (for e.g. as topics in a document). The entire dataset can therefore be modeled by a finite mixture of mixture of several probability distributions, e.g. a Gaussian mixture model of two components can be used to model the distribution of heights in males and females from the sampled data. To illustrate with an example, here we construct a one-dimensional Gaussian mixture model on the retention time (RT) of our peak features. Each mixture component ideally corresponds to a metabolite, since peaks that share close RT values should originate from the same metabolite. This can be represented as the weighted finite sum of its $K$ component distributions
\begin{equation}
p(\mathbf{y}|\boldsymbol{\mu},\boldsymbol{s},\boldsymbol{\pi})=\sum_{k=1}^{K}\pi_{k}\mathcal{{N}}(\mu_{k},s)
\end{equation}
where $\boldsymbol{\mu}=\{\mu_{1},...,\mu_{k}\}$ are the component means, $s$ is a fixed variance common to all components, and $\boldsymbol{\pi}=\{\pi_{1},...,\pi_{k}\}$ are the mixing proportions where $\pi_{k}>0$ and $\sum_{k=1}^{K}\pi_{k}=1$. The data points are represented as $\boldsymbol{y}=\{y_{1},y_{2},...,y_{n}\}$ where $y_{n}$ is the RT value of a peak feature. In this model, each retention time value is 'generated' by its $k$-th component Gaussian (a crucial modelling assumption here is all peaks are generated by a metabolite, which might not be true in the presence of noisy signals, ionisation products and other artefacts in the data). We denote this by the indicator variable $z_{nk}$ where $z_{nk}=1$ if feature $n$ is assigned to component $k$, and 0 otherwise. Collectively, the indicator variable $z_{nk}$ for all $n\in N$ and $k\in K$ can be stored inside the matrix $\mathbf{Z}$ of size $N$ by $K$. Let $\theta=\{\boldsymbol{\mu},\boldsymbol{\pi},\mathbf{Z}\}$ denotes all the parameters of interest in the model. We now have $p(\boldsymbol{y}|\lambda)$ and we would like to infer $p(\lambda|\boldsymbol{y})$, in particular all the indicator variables in $\mathbf{Z}$ that tells us which data point goes into which mixture component (cluster), i.e. the cluster memberships. We get this by applying Bayes' rule: 
\begin{equation}
p(\lambda|\boldsymbol{y})=\frac{p(\lambda)p(\boldsymbol{y}|\lambda)}{\int p(\lambda)p(\boldsymbol{y}|\lambda)d\lambda}
\end{equation}
The aim of inference here is to estimate model parameters from the posterior joint distribution $p(\lambda|\boldsymbol{y})$ of model parameters $\lambda$ given the data $\boldsymbol{y}$. For non-trivial models, this often involves solving the complex integration on the denumerator on the right hand side of the equation above, which can be difficult (it's not analytically tractable). Instead, parameter estimations can be done through maximum likelihood estimation (MLE), usually through the Expectation-Maximization algorithm, which finds model parameters maximising the likelihood of the model given the data. Alternatively, parameter estimations can also be done through Markov chain Monte Carlo (MCMC) methods. MCMC sampling allows us to approximate a target distribution via random walks obeying the Markovian property, where the current state in the random walk depends only on the previous state. When direct sampling of the posterior distribution is difficult but the full conditional distribution of each model parameter (e.g. $p(\mu_{k}|...)$ where ... denotes every other model parameter and the data) is easier to sample from, Gibbs sampling, an instance of MCMC methods, is often used. 

\section{Markov chain Monte Carlo methods}

In Gibbs sampling, each model parameter is sampled in turn from its full conditional distribution until the random walk converges to the target distribution. The advantage of MCMC methods is that we obtain distributions over the model parameters, which allow us to quantify our uncertainties on them -- as opposed to MLE method that provides only the most likely parameter values.

\section{Dirichlet Process mixture model clustering}

We can avoid specifying the number of cluster $K$ \emph{a-priori }by assuming that the data is generated by a mixture of infinite number of components (taking the limit as $K$ goes to $\infty$). Dirichlet Process is a stochastic process that describes a distribution of probabilty distributions, and is often used in Bayesian non-parameteric models -- particularly as a prior distribution in Dirichlet Process (DP) mixture model. In non-parametric models, the model structure (e.g. the number of mixture components) is not fixed in advance \textit{a priori}, but is instead determined based on the observed data. To do this, we place a Dirichlet process (DP) prior on the component parameters. Let $\theta_{k}$ denotes the component parameter of the $k$-th cluster. The DP can be viewed as an infinite dimensional generalisation of the Dirichlet distribution, where draws from the DP is itself a probability distribution. Following the example above, the RT data points can thus be explained by the following generative model:
\begin{align}
G|\alpha,H & \sim DP(\alpha,H)\\
\mu_{k}|G & \sim G\\
y_{n}|\mu_{k} & \sim{N}(\mu_{k},s)
\end{align}
Similar to the finite case, ${N}(\mu_{k},s)$ denotes the distribution of the data point $y_{n},$which is a Gaussian distribution parameterised by mean $\mu_{k}$ and variance $s$ (which is fixed, so we will not infer). The component parameter (the $\mu_{k}$s) are conditionally independent given $G$, which is a discrete distribution drawn from the Dirichlet Process, and the data point $y_{n}$ are conditionally independent given a component parameter $\mu_{k}$. $H$ is the base distribution that provides the prior on the $\mu_{k}$s, while the parameter $\alpha$ can be seen as the inverse variance, with larger values of $\alpha$ producing smaller variance in the distributions drawn from the GP from $H$. The DP prior induces a partitioning on the data points, where the probability of a newly arriving data point to join an existing cluster is proportional to the number of data points already in that cluster. However, with a probability proportional to $\alpha$, the data point will form a new cluster on its own. Additional details on Dirichlet process mixture model clustering can be found in \cite{Rasmussen2000}. 

\section{Hierarchical Dirichlet Process mixture model clustering}

While the DP mixture model allows us to cluster related peaks together within each run, we would also like such clusterings to be shared across runs. This is reasonable to expect because if a cluster represents related peaks derived from a metabolite / compound, then we can expect to discover the same clusters across similar runs. The idea here is that: (1) peaks put together in the same 'global' clusters are basically aligned, and (2) we can define a model with entities that are meaningful in the biological sense, and thus discover insights from such models.

Suppose we have $J$ runs to align. A Hierarchical Dirichlet process (HDP) is a distribution over a set of random probability measures, where each replicate has its associated random probability measure $G_{j}$. The global measure $G_{0}$ is distributed as a Dirichlet process, and the random measures $G_{j}$ for each replicate is also distributed according to a DP, conditionally independent on $G_{0}$
\begin{align}
G_{0}|\alpha,H & \sim DP(\alpha,H)\\
G_{j}|\alpha_{0},G_{0} & \sim DP(\alpha_{0},G_{0})\\
\mu_{k}|G_{j} & \sim G_{j}\\
y_{n}|\mu_{k} & \sim{N}(\mu_{k},s)
\end{align}
Notice that the difference between DP mixture and HDP mixture is the fact that we are adding another level of hierarchy to the model, where the probability measure $G_{j}$ for each replicate $j$ is in turn drawn from the global measure $G_{0}$. A draw from the DP is a discrete probability measure, so $G_{0}$ and $G_{j}$ are discrete distributions of point masses from the base distribution $H$. By drawing $G_{j}$, the $j$-th file specific measure, from a common global measure $G_{0}$, this makes it possible for us to share clustering parameters across different runs. In the popular Chinese Restaurant Franchise analogy described in \cite{Teh2005}, we have a Chinese restaurant franchise with a menu of dishes shared across all its restaurants (the global measure $G_{0}$). At each table (replicate cluster parameter) in a restaurant, one dish (global cluster parameter) is ordered from the menu by the first customer (data point) who sits there. The dish is shared by all customers who sit on that table. Newly arriving customer joins existing tables with a probability proportional to the number of people already sitting there, or sits on a new table by himself with a probability proportional to $\alpha_{0}$. Existing dishes are also ordered based on its popularity across the franchise (the number of tables ordering it), or a new dish is created with a probability proportional to $\alpha$. In this hierarchical DP process, cluster parameter values are shared across runs and also within run. 

\section{Latent Dirichet Allocation}

Latent Dirichlet Allocation (LDA), proposed in \cite{Blei2003}, is a probabilistic topic model widely used for unsupervised topic discovery. In the standard LDA model applied to text mining, documents comprise of some topics, each of which may produce the observed words in that document. Given a corpus of documents, the goal of inference in LDA is to approximate the posterior distributions of documents to topics and words to topics.

For the purpose of substructure discovery in MS2 data, a topic -- explained as the set of recurring words shared in many documents -- can be seen as corresponding to a substructure shared by many metabolites. Each topic then produces the observed MS2 fragment/loss words in an MS1 document. We assume the bag-of-word word model, where within each MS1 document, the observed MS2 fragment/loss word features are exchangeable. i.e. their ordering do not matter, only their observed counts matter. The input to LDA is therefore a matrix of the counts of occurences of MS2 word for each MS1 document. This can produced by concatenating the count matrices of the fragment words and the loss words produced in section \ref{sec:Feature-Extraction} row-wise, e.g. if there are $N_{f}$ unique fragment words and $N_{l}$ unique loss words, both of which are shared across $D$ MS1 peaks, the input matrix to LDA is a $D$-by-$(N_{f}+N_{l})$ matrix. Entries in the matrix are the observed counts of words in the document, so they are the discretised intensity values of the fragment and loss words for each MS1 peak -- produced according to Section \ref{sec:Feature-Extraction}. We restrict the input to the standard LDA to take into account only the fragment and loss words because the counts of both fragment and loss words are derived from the normalised intensity values of the MS2 peaks.

The standard LDA model -- as applied to substructure discovery -- is now briefly described here. Given $K$ predefined topics (indexed by $k=1,...,K$) corresponding to metabolite substructures, the observation of the $n$-th MS2 fragment/loss word in the $d$-th document (MS1 peak) can be described by the following generative process.
\begin{equation}
w_{dn}|\boldsymbol{\phi}_{z_{dn}}\sim Multinomial(\boldsymbol{\phi}_{z_{dn}})
\end{equation}
\begin{equation}
z_{dn}|\boldsymbol{\theta}_{d}\sim Multinomial(\boldsymbol{\theta}_{d})
\end{equation}
\begin{equation}
\boldsymbol{\theta}_{d}|\alpha\sim Dirichlet(\alpha)
\end{equation}
\begin{equation}
\boldsymbol{\phi}_{k}|\beta\sim Dirichlet(\beta)
\end{equation}
In other words: observation on the $n$-th MS2 word in the $d$-th MS1 peak ($w_{dn}$) is conditioned on the assignment of word $w_{dn}$ to some known $k$-th multinomial distribution (corresponding to a substructure). This assignment is denoted by the indicator variable $z_{dn}$, so $z_{dn}=k$ if $w_{dn}$ is assigned to a $k$-th multinomial. The $k$-th multinomial distribution that an MS2 word is assigned to is characterised by the parameter vector $\boldsymbol{\phi}_{z_{dn}}$. However, $\boldsymbol{\phi}_{z_{dn}}$ is itself drawn from a prior Dirichlet distribution having a symmetric parameter $\beta$. The probability of seeing certain substructures (topics) for each $d$-th MS1 peak is then drawn from a multinomial distribution with a parameter vector $\boldsymbol{\theta}_{d}$. This parameter vector $\boldsymbol{\theta}_{d}$ is in turn drawn from a prior Dirichlet distribution having a symmetric parameter $\alpha$. Figure \ref{fig-platediagram} is the plate diagram of the standard LDA model, which shows the conditional dependencies between the random variables in the model.